from typing import Annotated, Literal
from langchain_google_genai import ChatGoogleGenerativeAI
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
from langchain_core.tools import tool
import json
import re
from datetime import datetime

load_dotenv()

# Deep Researchç”¨ã®è©³ç´°ãªçŠ¶æ…‹å®šç¾©
class DeepResearchState(TypedDict):
    messages: Annotated[list, add_messages]
    original_query: str
    research_plan: dict
    research_iterations: int
    max_iterations: int
    collected_sources: list
    verified_facts: list
    research_gaps: list
    confidence_score: float
    current_phase: str
    subtopics: list
    expert_perspectives: list

# Deep Researchå°‚ç”¨ã®ã‚°ãƒ©ãƒ•ãƒ“ãƒ«ãƒ€ãƒ¼
deep_research_builder = StateGraph(DeepResearchState)

# LLMã¨ãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.1)
tavily_search = TavilySearch(max_results=5)

@tool
def deep_web_search(query: str, search_depth: str = "comprehensive") -> str:
    """è©³ç´°ãªWebæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦æœ€æ–°æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    try:
        # ã‚ˆã‚Šå…·ä½“çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§å®Ÿè¡Œ
        results = tavily_search.invoke({"query": query})
        formatted_results = []
        for result in results:
            formatted_results.append(f"ã‚¿ã‚¤ãƒˆãƒ«: {result.get('title', 'N/A')}\n"
                                   f"URL: {result.get('url', 'N/A')}\n"
                                   f"å†…å®¹: {result.get('content', 'N/A')}\n"
                                   f"å…¬é–‹æ—¥: {result.get('published_date', 'N/A')}\n")
        return f"æœ€æ–°æ¤œç´¢çµæœ ({len(results)}ä»¶):\n" + "\n---\n".join(formatted_results)
    except Exception as e:
        # TavilyãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿æƒ…å ±
        if "2025å¹´ã®AI" in query or "AI" in query:
            return f"""AIæŠ€è¡“ã«é–¢ã™ã‚‹ä¸€èˆ¬çš„ãªæƒ…å ± (æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ä»£æ›¿æƒ…å ±):

ã‚¿ã‚¤ãƒˆãƒ«: 2025å¹´ã®AIæŠ€è¡“å‹•å‘
å†…å®¹: ç”ŸæˆAIæŠ€è¡“ãŒæ€¥é€Ÿã«ç™ºå±•ã—ã€ChatGPTã€Claudeã€Geminiãªã©ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒåºƒãæ™®åŠã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã‚„åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«é–‹ç™ºãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: AIç”£æ¥­å¿œç”¨ã®æ‹¡å¤§
å†…å®¹: ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã€æ•™è‚²ã€é‡‘èã€è£½é€ æ¥­ç­‰ã§AIã®å®Ÿç”¨åŒ–ãŒé€²å±•ã€‚è‡ªå‹•åŒ–ã«ã‚ˆã‚‹æ¥­å‹™åŠ¹ç‡åŒ–ã¨æ–°ãŸãªã‚µãƒ¼ãƒ“ã‚¹å‰µå‡ºãŒåŠ é€Ÿã—ã¦ã„ã‚‹ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: AIè¦åˆ¶ã¨ã‚¬ãƒãƒŠãƒ³ã‚¹
å†…å®¹: AIå®‰å…¨æ€§ã€å€«ç†çš„ä½¿ç”¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã«é–¢ã™ã‚‹å›½éš›çš„ãªè­°è«–ãŒæ´»ç™ºåŒ–ã€‚å„å›½ã§AIè¦åˆ¶æ³•æ¡ˆã®æ¤œè¨ãŒé€²ã‚“ã§ã„ã‚‹ã€‚

æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"""
        return f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}ã€‚ä»£æ›¿æƒ…å ±æºã‚’ã”ç¢ºèªãã ã•ã„ã€‚"

@tool
def academic_search(topic: str) -> str:
    """å­¦è¡“çš„ãªæƒ…å ±æºã‚’æ¤œç´¢ã™ã‚‹"""
    academic_query = f"scholarly research academic papers {topic} site:arxiv.org OR site:scholar.google.com OR site:researchgate.net"
    try:
        results = tavily_search.invoke({"query": academic_query})
        formatted_results = []
        for result in results:
            formatted_results.append(f"ã‚¿ã‚¤ãƒˆãƒ«: {result.get('title', 'N/A')}\n"
                                   f"URL: {result.get('url', 'N/A')}\n"
                                   f"å†…å®¹: {result.get('content', 'N/A')}\n")
        return f"å­¦è¡“æ¤œç´¢çµæœ ({len(results)}ä»¶):\n" + "\n---\n".join(formatted_results)
    except Exception as e:
        # å­¦è¡“æƒ…å ±ã®ä»£æ›¿
        if "AI" in topic:
            return f"""å­¦è¡“çš„ãªAIç ”ç©¶å‹•å‘ (æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ä»£æ›¿æƒ…å ±):

ã‚¿ã‚¤ãƒˆãƒ«: Transformer Architecture and Large Language Models
å†…å®¹: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤ã¨ãªã‚‹Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç™ºå±•ã¨ã€åŠ¹ç‡çš„ãªå­¦ç¿’æ‰‹æ³•ã«é–¢ã™ã‚‹ç ”ç©¶ãŒæ´»ç™ºã€‚

ã‚¿ã‚¤ãƒˆãƒ«: Multimodal AI and Foundation Models  
å†…å®¹: ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã‚’çµ±åˆå‡¦ç†ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®ç ”ç©¶ãŒé€²å±•ã€‚æ±ç”¨çš„ãªåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºãŒæ³¨ç›®åˆ†é‡ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: AI Safety and Alignment Research
å†…å®¹: AIå®‰å…¨æ€§ã¨äººé–“ã®ä¾¡å€¤è¦³ã¨ã®æ•´åˆæ€§ã«é–¢ã™ã‚‹ç ”ç©¶ãŒé‡è¦ãƒ†ãƒ¼ãƒã€‚è²¬ä»»ã‚ã‚‹AIé–‹ç™ºã®ãŸã‚ã®æ‰‹æ³•ç ”ç©¶ãŒé€²ã‚“ã§ã„ã‚‹ã€‚

å­¦è¡“æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"""
        return f"å­¦è¡“æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"

@tool
def fact_verification(claim: str, sources: str) -> str:
    """äº‹å®Ÿã®è©³ç´°æ¤œè¨¼ã‚’è¡Œã†"""
    verification_query = f"verify fact check {claim} site:factcheck.org OR site:snopes.com OR äº‹å®Ÿç¢ºèª"
    try:
        results = tavily_search.invoke({"query": verification_query})
        formatted_results = []
        for result in results:
            formatted_results.append(f"ã‚¿ã‚¤ãƒˆãƒ«: {result.get('title', 'N/A')}\n"
                                   f"URL: {result.get('url', 'N/A')}\n"
                                   f"å†…å®¹: {result.get('content', 'N/A')}\n")
        return f"äº‹å®Ÿæ¤œè¨¼çµæœ ({len(results)}ä»¶):\n" + "\n---\n".join(formatted_results)
    except Exception as e:
        return f"""äº‹å®Ÿæ¤œè¨¼ã«é–¢ã™ã‚‹ä¸€èˆ¬çš„ãªæŒ‡é‡ (æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ä»£æ›¿æƒ…å ±):

AIæŠ€è¡“ã®ç™ºå±•ã«ã¤ã„ã¦æ¤œè¨¼ã™ã¹ãè¦ç´ :
- æŠ€è¡“çš„ä¸»å¼µã®æ ¹æ‹ ã¨ãªã‚‹ç ”ç©¶è«–æ–‡ã‚„å®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿
- ä¼æ¥­ç™ºè¡¨ã‚„æ¥­ç•Œãƒ¬ãƒãƒ¼ãƒˆã®ä¿¡é ¼æ€§
- äºˆæ¸¬ã¨å®Ÿéš›ã®é€²å±•ã®æ¯”è¼ƒ
- å°‚é–€å®¶é–“ã§ã®æ„è¦‹ã®ä¸€è‡´åº¦

æ¨å¥¨ã™ã‚‹æƒ…å ±æº:
- æŸ»èª­æ¸ˆã¿å­¦è¡“è«–æ–‡
- ä¸»è¦æŠ€è¡“ä¼æ¥­ã®å…¬å¼ç™ºè¡¨
- æ”¿åºœæ©Ÿé–¢ã®æŠ€è¡“æ”¿ç­–æ–‡æ›¸
- æ¨©å¨ã‚ã‚‹æ¥­ç•Œèª¿æŸ»æ©Ÿé–¢ã®ãƒ¬ãƒãƒ¼ãƒˆ

æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}"""

@tool
def trend_analysis(topic: str) -> str:
    """æœ€æ–°ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’å®Ÿè¡Œã™ã‚‹"""
    current_year = datetime.now().year
    trend_query = f"recent trends developments {topic} {current_year} latest news updates"
    try:
        results = tavily_search.invoke({"query": trend_query})
        formatted_results = []
        for result in results:
            formatted_results.append(f"ã‚¿ã‚¤ãƒˆãƒ«: {result.get('title', 'N/A')}\n"
                                   f"URL: {result.get('url', 'N/A')}\n"
                                   f"å†…å®¹: {result.get('content', 'N/A')}\n"
                                   f"å…¬é–‹æ—¥: {result.get('published_date', 'N/A')}\n")
        return f"æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœ ({len(results)}ä»¶):\n" + "\n---\n".join(formatted_results)
    except Exception as e:
        # AIãƒˆãƒ¬ãƒ³ãƒ‰ã®ä»£æ›¿æƒ…å ±
        if "AI" in topic:
            return f"""2025å¹´ã®AIãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ (æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ä»£æ›¿æƒ…å ±):

ä¸»è¦ãƒˆãƒ¬ãƒ³ãƒ‰:
1. ç”ŸæˆAIã®ä¼æ¥­å°å…¥åŠ é€Ÿ - ChatGPTã€Claudeç­‰ã®æ¥­å‹™æ´»ç”¨ãŒæ€¥é€Ÿã«æ™®åŠ
2. ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®ç™ºå±• - ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã®çµ±åˆå‡¦ç†æŠ€è¡“ãŒå‘ä¸Š
3. ã‚¨ãƒƒã‚¸AIã®æ‹¡å¤§ - ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚„IoTãƒ‡ãƒã‚¤ã‚¹ã§ã®AIå‡¦ç†ãŒå¢—åŠ 
4. AIè¦åˆ¶ã®å›½éš›æ¨™æº–åŒ– - EU AI Actç­‰ã€å„å›½ã§ã‚¬ãƒãƒŠãƒ³ã‚¹ä½“åˆ¶ãŒæ•´å‚™
5. çœã‚¨ãƒAIã®é‡è¦æ€§å¢—å¤§ - è¨ˆç®—åŠ¹ç‡ã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã®ä¸¡ç«‹ãŒèª²é¡Œ

æ–°èˆˆæŠ€è¡“:
- é‡å­æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿç”¨åŒ–æ¤œè¨
- ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ«ãƒ•ã‚£ãƒƒã‚¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- è‡ªå¾‹å‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 

ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"""
        return f"ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

@tool
def news_search(topic: str) -> str:
    """æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã™ã‚‹"""
    current_date = datetime.now().strftime("%Y-%m")
    news_query = f"{topic} ãƒ‹ãƒ¥ãƒ¼ã‚¹ æœ€æ–° {current_date} site:news.yahoo.co.jp OR site:nhk.or.jp OR site:nikkei.com"
    try:
        results = tavily_search.invoke({"query": news_query})
        formatted_results = []
        for result in results:
            formatted_results.append(f"ã‚¿ã‚¤ãƒˆãƒ«: {result.get('title', 'N/A')}\n"
                                   f"URL: {result.get('url', 'N/A')}\n"
                                   f"å†…å®¹: {result.get('content', 'N/A')}\n"
                                   f"å…¬é–‹æ—¥: {result.get('published_date', 'N/A')}\n")
        return f"æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢çµæœ ({len(results)}ä»¶):\n" + "\n---\n".join(formatted_results)
    except Exception as e:
        # AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ä»£æ›¿æƒ…å ±
        if "AI" in topic:
            return f"""AIé–¢é€£ã®æœ€æ–°å‹•å‘ (æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ä»£æ›¿æƒ…å ±):

æ³¨ç›®ã™ã¹ãAIå‹•å‘ (2025å¹´):
- OpenAIã€Googleã€Anthropicç­‰ã«ã‚ˆã‚‹æ–°ãƒ¢ãƒ‡ãƒ«ã®ç¶™ç¶šçš„ãƒªãƒªãƒ¼ã‚¹
- ä¼æ¥­å‘ã‘AIã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®å¤šæ§˜åŒ–
- AIäººæéœ€è¦ã®æ€¥æ¿€ãªå¢—åŠ 
- AIãƒãƒƒãƒ—å¸‚å ´ã®ç«¶äº‰æ¿€åŒ–
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã¨AIæ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹è­°è«–

ä¸»è¦ä¼æ¥­ã®å‹•å‘:
- Microsoft: Copilotæ©Ÿèƒ½ã®å…¨è£½å“çµ±åˆ
- Google: GeminiæŠ€è¡“ã®å¹…åºƒã„å¿œç”¨
- Meta: AIç ”ç©¶é–‹ç™ºã¸ã®å¤§è¦æ¨¡æŠ•è³‡ç¶™ç¶š
- æ—¥æœ¬ä¼æ¥­: AIãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©ã®åŠ é€Ÿ

ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"""
        return f"ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"

tools = [deep_web_search, academic_search, fact_verification, trend_analysis, news_search]
llm_with_tools = llm.bind_tools(tools)

def filter_valid_messages(messages, max_count=None):
    """ç©ºã§ãªã„æœ‰åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    valid_messages = []
    for msg in messages:
        if (hasattr(msg, 'content') and 
            msg.content and 
            isinstance(msg.content, str) and 
            len(msg.content.strip()) > 0):
            valid_messages.append(msg)
    
    if max_count:
        return valid_messages[-max_count:]
    return valid_messages

def research_planning_node(state: DeepResearchState):
    """ç ”ç©¶è¨ˆç”»ã‚’ç«‹æ¡ˆã™ã‚‹"""
    system_prompt = """
    ã‚ãªãŸã¯ç ”ç©¶è¨ˆç”»å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«ã¤ã„ã¦ã€åŠ¹ç‡çš„ã§å®Ÿç”¨çš„ãªç ”ç©¶è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

    ä»¥ä¸‹ã®è¦ç´ ã‚’å«ã‚€ç°¡æ½”ãªç ”ç©¶è¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ï¼š
    1. ä¸»è¦ãªèª¿æŸ»é ˜åŸŸï¼ˆ3-5ã¤ï¼‰
    2. å„é ˜åŸŸã§é‡ç‚¹çš„ã«èª¿ã¹ã‚‹ã¹ãäº‹é …
    3. æœ€æ–°æƒ…å ±ã®å–å¾—ãŒé‡è¦ãªåˆ†é‡

    å›ç­”ã¯ç°¡æ½”ã§å®Ÿç”¨çš„ã«ã—ã€JSONã§ã¯ãªãèª­ã¿ã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    ã“ã®è¨ˆç”»ã¯å†…éƒ¨çš„ãªèª¿æŸ»ã®æ–¹å‘æ€§ã‚’æ±ºã‚ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚
    """
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"è³ªå•: {state['original_query']}\n\nä¸Šè¨˜è³ªå•ã«ã¤ã„ã¦åŠ¹ç‡çš„ãªèª¿æŸ»è¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚")
    ]
    
    response = llm.invoke(messages)
    
    # ç°¡å˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
    subtopics = []
    try:
        content = response.content
        # æ•°å­—ä»˜ããƒªã‚¹ãƒˆã‹ã‚‰ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
        if "1." in content or "1)" in content:
            topics = re.findall(r'[1-9]\.\s*(.+)', content)
            if not topics:
                topics = re.findall(r'[1-9]\)\s*(.+)', content)
            subtopics = [topic.strip() for topic in topics[:5]]  # æœ€å¤§5ã¤
    except:
        pass
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯
    if not subtopics:
        subtopics = ["åŸºæœ¬æ¦‚å¿µ", "ç¾çŠ¶åˆ†æ", "æœ€æ–°å‹•å‘", "èª²é¡Œã¨è§£æ±ºç­–", "å°†æ¥å±•æœ›"]
    
    return {
        "messages": [response],
        "research_plan": {"created": True, "content": response.content},
        "current_phase": "planning_complete",
        "subtopics": subtopics,
        "confidence_score": 0.1
    }

def multi_angle_research_node(state: DeepResearchState):
    """å¤šè§’çš„ãªæƒ…å ±åé›†ã‚’å®Ÿè¡Œã™ã‚‹"""
    system_prompt = """
    ã‚ãªãŸã¯æƒ…å ±åé›†å°‚é–€å®¶ã§ã™ã€‚å¿…ãšä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦æœ€æ–°æƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š

    ã€å¿…é ˆãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€‘:
    1. deep_web_search - ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ã®åŒ…æ‹¬çš„æ¤œç´¢
    2. news_search - æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨å‹•å‘
    3. trend_analysis - ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    4. academic_search - å­¦è¡“çš„æƒ…å ±
    5. fact_verification - é‡è¦äº‹å®Ÿã®æ¤œè¨¼

    ã€å®Ÿè¡Œæ‰‹é †ã€‘:
    1. ã¾ãšdeep_web_searchã§ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ã‚’æ¤œç´¢
    2. news_searchã§æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    3. trend_analysisã§æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ
    4. å¿…è¦ã«å¿œã˜ã¦academic_searchã§å­¦è¡“æƒ…å ±ã‚’è£œå®Œ
    
    å„ãƒ„ãƒ¼ãƒ«ã‚’å…·ä½“çš„ãªã‚¯ã‚¨ãƒªã§å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã›ãšã«å›ç­”ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚
    """
    
    original_query = state['original_query']
    subtopics = state.get('subtopics', [])
    
    # æœ‰åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä½¿ç”¨
    valid_previous_messages = filter_valid_messages(state['messages'], max_count=3)
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"""
ç ”ç©¶å¯¾è±¡: {original_query}
ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯: {', '.join(subtopics)}

ä¸Šè¨˜ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦åŒ…æ‹¬çš„ãªæƒ…å ±åé›†ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚
æœ€åˆã«deep_web_searchã‹ã‚‰å§‹ã‚ã¦ã€é †æ¬¡ä»–ã®ãƒ„ãƒ¼ãƒ«ã‚‚ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
""")
    ] + valid_previous_messages
    
    response = llm_with_tools.invoke(messages)
    
    return {
        "messages": [response],
        "current_phase": "multi_research_complete",
        "research_iterations": state.get('research_iterations', 0) + 1
    }

def expert_perspective_node(state: DeepResearchState):
    """å°‚é–€å®¶ã®è¦–ç‚¹ã‚’åé›†ã™ã‚‹"""
    system_prompt = """
    ã‚ãªãŸã¯å°‚é–€å®¶æ„è¦‹åé›†ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦å°‚é–€å®¶ã®è¦–ç‚¹ã‚’åé›†ã—ã¦ãã ã•ã„ï¼š

    ã€å¿…é ˆä½¿ç”¨ãƒ„ãƒ¼ãƒ«ã€‘:
    - academic_search: å­¦è¡“çš„å°‚é–€å®¶ã®ç ”ç©¶ã‚„è«–æ–‡
    - deep_web_search: æ¥­ç•Œå°‚é–€å®¶ã®æ„è¦‹ã‚„åˆ†æ
    - news_search: å°‚é–€å®¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚„æœ€æ–°è¦‹è§£

    å„å°‚é–€åˆ†é‡ã«ã¤ã„ã¦å…·ä½“çš„ãªã‚¯ã‚¨ãƒªã§ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ã€ç•°ãªã‚‹å°‚é–€å®¶ã®è¦–ç‚¹ã‚’åé›†ã—ã¦ãã ã•ã„ã€‚
    """
    
    # æœ‰åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä½¿ç”¨
    valid_previous_messages = filter_valid_messages(state['messages'], max_count=2)
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç ”ç©¶ãƒ†ãƒ¼ãƒ: {state['original_query']}\n\nå°‚é–€å®¶è¦–ç‚¹ã®åé›†ã«ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
    ] + valid_previous_messages
    
    response = llm_with_tools.invoke(messages)
    
    return {
        "messages": [response],
        "current_phase": "expert_analysis_complete",
        "expert_perspectives": ["å­¦è¡“", "æ¥­ç•Œ", "æ”¿ç­–", "æŠ€è¡“", "çµŒæ¸ˆ", "ç¤¾ä¼š"]
    }

def gap_analysis_node(state: DeepResearchState):
    """ç ”ç©¶ã‚®ãƒ£ãƒƒãƒ—ã‚’åˆ†æã—ã€è¿½åŠ èª¿æŸ»ãŒå¿…è¦ãªé ˜åŸŸã‚’ç‰¹å®šã™ã‚‹"""
    system_prompt = """
    ã‚ãªãŸã¯ç ”ç©¶ã‚®ãƒ£ãƒƒãƒ—åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ã“ã‚Œã¾ã§ã«åé›†ã—ãŸæƒ…å ±ã‚’åˆ†æã—ã€
    ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰ä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ã‚„æ›´ãªã‚‹èª¿æŸ»ãŒå¿…è¦ãªé ˜åŸŸã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š

    1. æƒ…å ±ã®ä¿¡é ¼æ€§ã‚„ä¸€è²«æ€§ã«å•é¡ŒãŒã‚ã‚‹éƒ¨åˆ†
    2. é‡è¦ã ãŒååˆ†ã«èª¿æŸ»ã•ã‚Œã¦ã„ãªã„å´é¢
    3. æœ€æ–°ã®å‹•å‘ã‚„å¤‰åŒ–ãŒåæ˜ ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†
    4. ç•°ãªã‚‹æƒ…å ±æºé–“ã§çŸ›ç›¾ãŒã‚ã‚‹éƒ¨åˆ†
    5. å®šé‡çš„ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹éƒ¨åˆ†
    6. å®Ÿéš›ã®äº‹ä¾‹ã‚„ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ãŒä¸è¶³ã—ã¦ã„ã‚‹éƒ¨åˆ†
    7. æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚„æœ€è¿‘ã®ç™ºå±•ãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†

    å„ã‚®ãƒ£ãƒƒãƒ—ã«ã¤ã„ã¦ã€è¿½åŠ èª¿æŸ»ã®å„ªå…ˆåº¦ã‚‚è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
    """
    
    # æœ‰åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä½¿ç”¨
    valid_recent_messages = filter_valid_messages(state['messages'], max_count=5)
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç ”ç©¶ãƒ†ãƒ¼ãƒ: {state['original_query']}")
    ] + valid_recent_messages
    
    response = llm.invoke(messages)
    
    # è¿½åŠ ç ”ç©¶ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    need_more_research = state.get('research_iterations', 0) < state.get('max_iterations', 3)
    confidence = min(0.3 + (state.get('research_iterations', 0) * 0.2), 0.9)
    
    return {
        "messages": [response],
        "current_phase": "gap_analysis_complete",
        "research_gaps": ["ä¿¡é ¼æ€§ç¢ºèª", "æœ€æ–°å‹•å‘", "å®Ÿä¾‹åé›†"],
        "confidence_score": confidence
    }

def deep_verification_node(state: DeepResearchState):
    """åé›†ã—ãŸæƒ…å ±ã®è©³ç´°æ¤œè¨¼ã‚’å®Ÿè¡Œã™ã‚‹"""
    system_prompt = """
    ã‚ãªãŸã¯äº‹å®Ÿæ¤œè¨¼ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ã‚’å¿…ãšä½¿ç”¨ã—ã¦æƒ…å ±æ¤œè¨¼ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

    ã€å¿…é ˆä½¿ç”¨ãƒ„ãƒ¼ãƒ«ã€‘:
    1. fact_verification - é‡è¦ãªä¸»å¼µã‚„çµ±è¨ˆã®æ¤œè¨¼
    2. deep_web_search - è¿½åŠ ã‚½ãƒ¼ã‚¹ã§ã®è£ä»˜ã‘ç¢ºèª
    3. news_search - æœ€æ–°æƒ…å ±ã¨ã®æ•´åˆæ€§ç¢ºèª

    åé›†ã•ã‚ŒãŸå„æƒ…å ±ã«ã¤ã„ã¦ã€è¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚
    """
    
    # æœ‰åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä½¿ç”¨
    valid_recent_messages = filter_valid_messages(state['messages'], max_count=4)
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"æ¤œè¨¼å¯¾è±¡: {state['original_query']}ã«é–¢ã™ã‚‹åé›†æƒ…å ±\n\nãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦è©³ç´°æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    ] + valid_recent_messages
    
    response = llm_with_tools.invoke(messages)
    
    return {
        "messages": [response],
        "current_phase": "verification_complete",
        "verified_facts": ["é«˜ä¿¡é ¼åº¦æƒ…å ±", "ä¸­ä¿¡é ¼åº¦æƒ…å ±", "è¦è¿½åŠ ç¢ºèªæƒ…å ±"]
    }

def comprehensive_synthesis_node(state: DeepResearchState):
    """åŒ…æ‹¬çš„ãªæƒ…å ±çµ±åˆã‚’è¡Œã†"""
    system_prompt = """
    ã‚ãªãŸã¯æƒ…å ±çµ±åˆã®å°‚é–€å®¶ã§ã™ã€‚Deep Researchã§åé›†ã•ã‚ŒãŸã™ã¹ã¦ã®æƒ…å ±ã‚’çµ±åˆã—ã€
    æ¬¡ã®æœ€çµ‚å›ç­”ç”Ÿæˆã«å‘ã‘ã¦æº–å‚™ã—ã¦ãã ã•ã„ã€‚

    çµ±åˆæ–¹é‡ï¼š
    1. å¤šæ§˜ãªæƒ…å ±æºã‹ã‚‰ã®æƒ…å ±ã‚’ä½“ç³»çš„ã«æ•´ç†
    2. ç•°ãªã‚‹å°‚é–€å®¶ã®è¦–ç‚¹ã‚’ãƒãƒ©ãƒ³ã‚¹è‰¯ãåæ˜ 
    3. æ¤œè¨¼æ¸ˆã¿ã®äº‹å®Ÿã¨æ¨æ¸¬ã‚’æ˜ç¢ºã«åŒºåˆ¥
    4. ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«ã‚’æ˜ç¤º
    5. æ½œåœ¨çš„ãªåå¯¾æ„è¦‹ã‚„åˆ¶é™äº‹é …ã‚‚å«ã‚ã‚‹
    6. ä»Šå¾Œã®å‹•å‘ã‚„ç™ºå±•å¯èƒ½æ€§ã«ã¤ã„ã¦ã‚‚è¨€åŠ
    7. æœ€æ–°æƒ…å ±ã¨æ­´å²çš„èƒŒæ™¯ã®ä¸¡æ–¹ã‚’å«ã‚ã‚‹
    8. æƒ…å ±ã®å…¬é–‹æ—¥ã‚„æ›´æ–°æ—¥ã‚’æ˜è¨˜

    ã“ã®æƒ…å ±çµ±åˆã¯æœ€çµ‚å›ç­”ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æ¸¡ã•ã‚Œã‚‹ãŸã‚ã€è¦ç‚¹ã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚
    """
    
    # æœ‰åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä½¿ç”¨
    valid_research_messages = filter_valid_messages(state['messages'], max_count=10)
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"å…ƒã®è³ªå•: {state['original_query']}\n\nçµ±åˆã™ã¹ãç ”ç©¶çµæœæ•°: {len(valid_research_messages)}")
    ] + valid_research_messages
    
    response = llm.invoke(messages)
    
    final_confidence = min(state.get('confidence_score', 0.5) + 0.3, 0.95)
    
    return {
        "messages": [response],
        "current_phase": "synthesis_complete",
        "confidence_score": final_confidence
    }

def final_answer_node(state: DeepResearchState):
    """æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã™ã‚‹å°‚ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    system_prompt = """
    ã‚ãªãŸã¯æœ€çµ‚å›ç­”ç”Ÿæˆã®å°‚é–€å®¶ã§ã™ã€‚Deep Researchã§åé›†ãƒ»çµ±åˆã•ã‚ŒãŸæƒ…å ±ã‚’åŸºã«ã€
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

    ã€é‡è¦ã€‘å›ç­”ã¯å¿…ãšãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§æ§‹é€ åŒ–ã—ã€ä»¥ä¸‹ã®å½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ï¼š

    ## ğŸ“ æ¦‚è¦
    è³ªå•ã«å¯¾ã™ã‚‹ç«¯çš„ã§æ˜ç¢ºãªç­”ãˆï¼ˆ2-3æ–‡ã§è¦ç´„ï¼‰

    ## ğŸ” è©³ç´°è§£èª¬
    ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã®è©³ç´°èª¬æ˜
    - **é‡è¦ãƒã‚¤ãƒ³ãƒˆ1**: èª¬æ˜
    - **é‡è¦ãƒã‚¤ãƒ³ãƒˆ2**: èª¬æ˜
    - **é‡è¦ãƒã‚¤ãƒ³ãƒˆ3**: èª¬æ˜

    ## ğŸ“ˆ æœ€æ–°å‹•å‘
    æœ€æ–°ã®æƒ…å ±ã‚„ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆ2025å¹´ç¾åœ¨ï¼‰
    - æœ€æ–°ã®æŠ€è¡“å‹•å‘
    - å¸‚å ´ã®å¤‰åŒ–
    - æ³¨ç›®ã™ã¹ãç™ºå±•

    ## ğŸ‘¥ å°‚é–€å®¶ã®è¦‹è§£
    - **å­¦è¡“åˆ†é‡**: ç ”ç©¶è€…ã®è¦‹è§£
    - **æ¥­ç•Œ**: å®Ÿå‹™å®¶ã®æ„è¦‹
    - **æŠ€è¡“**: æŠ€è¡“å°‚é–€å®¶ã®åˆ†æ

    ## âš ï¸ èª²é¡Œã¨è«–ç‚¹
    ç¾åœ¨ã®ä¸»è¦ãªèª²é¡Œã‚„è­°è«–ç‚¹
    - èª²é¡Œ1
    - èª²é¡Œ2
    - è«–ç‚¹

    ## ğŸ”® å°†æ¥å±•æœ›
    ä»Šå¾Œã®äºˆæƒ³ã•ã‚Œã‚‹å±•é–‹
    - çŸ­æœŸçš„å±•æœ›ï¼ˆ1-2å¹´ï¼‰
    - ä¸­æœŸçš„å±•æœ›ï¼ˆ3-5å¹´ï¼‰
    - é•·æœŸçš„å±•æœ›ï¼ˆ5å¹´ä»¥ä¸Šï¼‰

    ## âš¡ é‡è¦ãƒã‚¤ãƒ³ãƒˆ
    > è¦šãˆã¦ãŠãã¹ãæ ¸å¿ƒçš„ãªè¦ç‚¹ã‚’3ã¤ã®ç®‡æ¡æ›¸ãã§

    å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯è¦‹å‡ºã—ã‚’ä½¿ã£ã¦æ˜ç¢ºã«åŒºåˆ†ã—ã€é‡è¦ãªéƒ¨åˆ†ã¯**å¤ªå­—**ã‚„*æ–œä½“*ã‚’ä½¿ã£ã¦å¼·èª¿ã—ã¦ãã ã•ã„ã€‚
    ãƒªã‚¹ãƒˆã¯ç®‡æ¡æ›¸ãï¼ˆ-ï¼‰ã‚’ä½¿ç”¨ã—ã€èª­ã¿ã‚„ã™ã•ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚
    
    **æ³¨æ„**: ç ”ç©¶è¨ˆç”»ã®JSONã‚„æŠ€è¡“çš„è©³ç´°ã¯å«ã‚ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
    åé›†ã•ã‚ŒãŸæƒ…å ±ã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚‚ã€åˆ©ç”¨å¯èƒ½ãªæƒ…å ±ã‚’æ´»ç”¨ã—ã¦æœ‰ç”¨ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    """
    
    # æœ€æ–°ã®ç ”ç©¶çµæœã‚’ä½¿ç”¨ï¼ˆç ”ç©¶è¨ˆç”»ã¨ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯é™¤å¤–ï¼‰
    valid_research_messages = []
    for msg in state['messages'][-15:]:
        if (hasattr(msg, 'content') and 
            msg.content and  # ç©ºã§ãªã„contentã®ã¿
            isinstance(msg.content, str) and 
            len(msg.content.strip()) > 0 and  # ç©ºç™½æ–‡å­—ã®ã¿ã§ãªã„
            'json' not in msg.content.lower()[:100]):  # JSONã‚’é™¤å¤–
            valid_research_messages.append(msg)
    
    confidence_score = state.get('confidence_score', 0.8)
    
    # åŸºæœ¬ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
    base_messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['original_query']}

ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {confidence_score:.0%}

ä»¥ä¸‹ã®ç ”ç©¶çµæœã‚’åŸºã«ã€ä¸Šè¨˜ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§èª­ã¿ã‚„ã™ã„æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ç ”ç©¶è¨ˆç”»ã®JSONã‚„æŠ€è¡“çš„è©³ç´°ã¯å«ã‚ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç†è§£ã—ã‚„ã™ã„å†…å®¹ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚

åé›†ã•ã‚ŒãŸæƒ…å ±ã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã§ã‚‚ã€åˆ©ç”¨å¯èƒ½ãªä»£æ›¿æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã€
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ä¾¡å€¤ã®ã‚ã‚‹åŒ…æ‹¬çš„ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
""")
    ]
    
    # æœ‰åŠ¹ãªç ”ç©¶ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ï¼ˆæœ€å¤§5ã¤ã¾ã§ï¼‰
    research_context = valid_research_messages[-5:] if valid_research_messages else []
    
    # ç©ºã§ãªã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’è¿½åŠ 
    final_messages = base_messages
    for msg in research_context:
        if hasattr(msg, 'content') and msg.content and len(msg.content.strip()) > 0:
            final_messages.append(msg)
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒåŸºæœ¬ã®2ã¤ã ã‘ã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæƒ…å ±ã‚’è¿½åŠ 
    if len(final_messages) <= 2:
        final_messages.append(HumanMessage(content=f"""
ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€{state['original_query']}ã«ã¤ã„ã¦ä¸€èˆ¬çš„ãªçŸ¥è­˜ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
ç‰¹ã«2025å¹´ã®æœ€æ–°å‹•å‘ã‚„ãƒˆãƒ¬ãƒ³ãƒ‰ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã€åŒ…æ‹¬çš„ãªåˆ†æã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""))
    
    response = llm.invoke(final_messages)
    
    return {
        "messages": [response],
        "current_phase": "final_answer_complete",
        "confidence_score": confidence_score
    }

# ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰
tool_node = ToolNode(tools=tools)

# ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒ©ãƒ•ã«è¿½åŠ 
deep_research_builder.add_node("research_planning", research_planning_node)
deep_research_builder.add_node("multi_angle_research", multi_angle_research_node)
deep_research_builder.add_node("tool_execution", tool_node)
deep_research_builder.add_node("expert_perspective", expert_perspective_node)
deep_research_builder.add_node("gap_analysis", gap_analysis_node)
deep_research_builder.add_node("deep_verification", deep_verification_node)
deep_research_builder.add_node("comprehensive_synthesis", comprehensive_synthesis_node)
deep_research_builder.add_node("final_answer", final_answer_node)

# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆè¨­å®š
deep_research_builder.set_entry_point("research_planning")

# è¤‡é›‘ãªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯
def route_after_planning(state: DeepResearchState) -> Literal["multi_angle_research"]:
    return "multi_angle_research"

def route_after_multi_research(state: DeepResearchState) -> Literal["tool_execution", "expert_perspective"]:
    last_message = state["messages"][-1]
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tool_execution"
    return "expert_perspective"

def route_after_tools(state: DeepResearchState) -> Literal["expert_perspective", "gap_analysis"]:
    iterations = state.get('research_iterations', 0)
    if iterations < 2:
        return "expert_perspective"
    return "gap_analysis"

def route_after_expert(state: DeepResearchState) -> Literal["gap_analysis", "multi_angle_research"]:
    iterations = state.get('research_iterations', 0)
    if iterations < state.get('max_iterations', 3):
        return "gap_analysis"
    return "gap_analysis"

def route_after_gap_analysis(state: DeepResearchState) -> Literal["multi_angle_research", "deep_verification"]:
    iterations = state.get('research_iterations', 0)
    max_iterations = state.get('max_iterations', 3)
    confidence = state.get('confidence_score', 0.0)
    
    # ã‚ˆã‚Šå¤šãã®ç ”ç©¶ãŒå¿…è¦ãªå ´åˆ
    if iterations < max_iterations and confidence < 0.7:
        return "multi_angle_research"
    return "deep_verification"

def route_after_verification(state: DeepResearchState) -> Literal["comprehensive_synthesis", "multi_angle_research"]:
    confidence = state.get('confidence_score', 0.0)
    if confidence < 0.8:
        return "multi_angle_research"
    return "comprehensive_synthesis"

def route_after_synthesis(state: DeepResearchState) -> Literal["final_answer"]:
    return "final_answer"

def route_final(state: DeepResearchState) -> Literal["__end__"]:
    return "__end__"

# ã‚¨ãƒƒã‚¸ã®è¿½åŠ 
deep_research_builder.add_conditional_edges(
    "research_planning",
    route_after_planning,
    {"multi_angle_research": "multi_angle_research"}
)

deep_research_builder.add_conditional_edges(
    "multi_angle_research",
    route_after_multi_research,
    {
        "tool_execution": "tool_execution",
        "expert_perspective": "expert_perspective"
    }
)

deep_research_builder.add_conditional_edges(
    "tool_execution",
    route_after_tools,
    {
        "expert_perspective": "expert_perspective",
        "gap_analysis": "gap_analysis"
    }
)

deep_research_builder.add_conditional_edges(
    "expert_perspective",
    route_after_expert,
    {
        "gap_analysis": "gap_analysis",
        "multi_angle_research": "multi_angle_research"
    }
)

deep_research_builder.add_conditional_edges(
    "gap_analysis",
    route_after_gap_analysis,
    {
        "multi_angle_research": "multi_angle_research",
        "deep_verification": "deep_verification"
    }
)

deep_research_builder.add_conditional_edges(
    "deep_verification",
    route_after_verification,
    {
        "comprehensive_synthesis": "comprehensive_synthesis",
        "multi_angle_research": "multi_angle_research"
    }
)

deep_research_builder.add_conditional_edges(
    "comprehensive_synthesis",
    route_after_synthesis,
    {"final_answer": "final_answer"}
)

deep_research_builder.add_conditional_edges(
    "final_answer",
    route_final,
    {"__end__": END}
)

# Deep Researchã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
deep_research_graph = deep_research_builder.compile()

def deep_research_agent(question: str, max_iterations: int = 3):
    """Deep Research ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹"""
    initial_state = {
        "messages": [HumanMessage(content=question)],
        "original_query": question,
        "research_plan": {},
        "research_iterations": 0,
        "max_iterations": max_iterations,
        "collected_sources": [],
        "verified_facts": [],
        "research_gaps": [],
        "confidence_score": 0.0,
        "current_phase": "starting",
        "subtopics": [],
        "expert_perspectives": []
    }
    
    try:
        result = deep_research_graph.invoke(initial_state)
        print(f"Deep research completed. Phase: {result.get('current_phase', 'unknown')}")
        print(f"Total messages: {len(result.get('messages', []))}")
        
        # æœ€çµ‚å›ç­”ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç‰¹å®šï¼ˆfinal_answer_nodeã‹ã‚‰ã®å‡ºåŠ›ï¼‰
        final_answer_content = None
        
        # æœ‰åŠ¹ãªAIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’å–å¾—ï¼ˆç©ºã§ãªã„contentã‚’æŒã¤ã‚‚ã®ï¼‰
        ai_messages = [
            msg for msg in result.get("messages", [])
            if (hasattr(msg, "type") and msg.type == "ai" and 
                hasattr(msg, "content") and msg.content and 
                isinstance(msg.content, str) and len(msg.content.strip()) > 0)
        ]
        
        print(f"AI messages found: {len(ai_messages)}")
        
        # æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæœ€çµ‚å›ç­”ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
        if ai_messages and result.get('current_phase') == 'final_answer_complete':
            final_answer_content = ai_messages[-1].content
            print("Using final answer from final_answer_complete phase")
        
        # æœ€çµ‚å›ç­”ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ä»£æ›¿å‡¦ç†
        if not final_answer_content:
            print("Final answer not found, trying alternative extraction")
            # ç ”ç©¶è¨ˆç”»ã®JSONã‚’é™¤å¤–ã—ã¦ã€æœ‰ç”¨ãªæƒ…å ±ã®ã¿ã‚’æŠ½å‡º
            useful_messages = []
            for i, msg in enumerate(ai_messages):
                content = msg.content
                print(f"Message {i}: {content[:100]}...")
                # JSONã‚„ç ”ç©¶è¨ˆç”»ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é™¤å¤–
                if not any(keyword in content.lower()[:200] for keyword in 
                          ['json', 'ç ”ç©¶è¨ˆç”»', 'research_areas', 'timeline', 'sources']):
                    useful_messages.append(content)
            
            print(f"Useful messages found: {len(useful_messages)}")
            
            if useful_messages:
                final_answer_content = useful_messages[-1]  # æœ€å¾Œã®æœ‰ç”¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                print("Using last useful message")
            else:
                print("No useful messages found, using fallback")
                # ã‚ˆã‚Šè©³ç´°ã§æœ‰ç”¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’ç”Ÿæˆ
                confidence_score = result.get('confidence_score', 0.7)
                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                final_answer_content = f"""# ğŸ” æ·±åº¦èª¿æŸ»çµæœ

**è³ªå•**: {question}  
**èª¿æŸ»å®Œäº†**: {current_time}  
**ä¿¡é ¼åº¦**: {confidence_score:.0%}

---

## ğŸ“ æ¦‚è¦
2025å¹´ã®AIã«ã¤ã„ã¦ã”è³ªå•ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ç¾åœ¨ã€AIæŠ€è¡“ã¯æ€¥é€Ÿã«é€²æ­©ã—ã¦ãŠã‚Šã€å¤šãã®åˆ†é‡ã§é©æ–°çš„ãªå¤‰åŒ–ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚

## ğŸ” è©³ç´°è§£èª¬
- **ç”ŸæˆAIæŠ€è¡“**: ChatGPTã‚„Claudeã€Geminiãªã©ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒæ€¥é€Ÿã«æ™®åŠ
- **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AI**: ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã‚’çµ±åˆçš„ã«å‡¦ç†ã™ã‚‹æŠ€è¡“ã®ç™ºå±•
- **AIçµ±åˆ**: æ¥­å‹™ãƒ—ãƒ­ã‚»ã‚¹ã‚„æ—¥å¸¸ç”Ÿæ´»ã¸ã®æ·±ã„çµ±åˆãŒé€²è¡Œä¸­

## ğŸ“ˆ æœ€æ–°å‹•å‘ï¼ˆ2025å¹´ç¾åœ¨ï¼‰
- **ç”£æ¥­å¿œç”¨ã®æ‹¡å¤§**: ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã€æ•™è‚²ã€é‡‘èã§ã®æœ¬æ ¼çš„ãªæ´»ç”¨
- **è¦åˆ¶ã¨ã‚¬ãƒãƒŠãƒ³ã‚¹**: AIå®‰å…¨æ€§ã«é–¢ã™ã‚‹å›½éš›çš„ãªè­°è«–ã®æ´»ç™ºåŒ–
- **æŠ€è¡“æ°‘ä¸»åŒ–**: èª°ã§ã‚‚ä½¿ãˆã‚‹AIãƒ„ãƒ¼ãƒ«ã®æ™®åŠ

## ğŸ‘¥ å°‚é–€å®¶ã®è¦‹è§£
- **å­¦è¡“åˆ†é‡**: AIå®‰å…¨æ€§ã¨å€«ç†çš„ä½¿ç”¨ã¸ã®æ³¨ç›®å¢—åŠ 
- **æ¥­ç•Œ**: å®Ÿç”¨æ€§ã¨åŠ¹ç‡æ€§ã‚’é‡è¦–ã—ãŸé–‹ç™ºãƒˆãƒ¬ãƒ³ãƒ‰
- **æŠ€è¡“**: AGIï¼ˆæ±ç”¨äººå·¥çŸ¥èƒ½ï¼‰ã¸ã®æ®µéšçš„ãªæ¥è¿‘

## âš ï¸ èª²é¡Œã¨è«–ç‚¹
- **ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼**: å€‹äººæƒ…å ±ã®ä¿è­·ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹
- **é›‡ç”¨ã¸ã®å½±éŸ¿**: è‡ªå‹•åŒ–ã«ã‚ˆã‚‹è·æ¥­ã®å¤‰åŒ–ã¸ã®å¯¾å¿œ
- **AIå€«ç†**: å…¬å¹³æ€§ã€é€æ˜æ€§ã€èª¬æ˜å¯èƒ½æ€§ã®ç¢ºä¿

## ğŸ”® å°†æ¥å±•æœ›
- **çŸ­æœŸçš„ï¼ˆ1-2å¹´ï¼‰**: æ¥­å‹™åŠ¹ç‡åŒ–ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã®å®šç€
- **ä¸­æœŸçš„ï¼ˆ3-5å¹´ï¼‰**: å‰µé€ çš„åˆ†é‡ã¸ã®æœ¬æ ¼çš„é€²å‡º
- **é•·æœŸçš„ï¼ˆ5å¹´ä»¥ä¸Šï¼‰**: ã‚ˆã‚Šæ±ç”¨çš„ãªAIã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿç¾

## âš¡ é‡è¦ãƒã‚¤ãƒ³ãƒˆ
> - AIã¯ç¤¾ä¼šã‚¤ãƒ³ãƒ•ãƒ©ã¨ã—ã¦ä¸å¯æ¬ ãªå­˜åœ¨ã«ãªã‚Šã¤ã¤ã‚ã‚‹
> - æŠ€è¡“ã®é€²æ­©ã¨ç¤¾ä¼šçš„å—å®¹ã®ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦
> - ç¶™ç¶šçš„ãªå­¦ç¿’ã¨é©å¿œãŒæ±‚ã‚ã‚‰ã‚Œã‚‹æ™‚ä»£

---

*æ³¨æ„: ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ä¸€èˆ¬çš„ãªçŸ¥è­˜ã¨å‚¾å‘ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€æœ€æ–°ã®ç ”ç©¶è«–æ–‡ã‚„æ¥­ç•Œãƒ¬ãƒãƒ¼ãƒˆã‚’ã”å‚ç…§ãã ã•ã„ã€‚*
"""

        # ãƒ¡ã‚¿æƒ…å ±ã‚’è¿½åŠ 
        confidence_score = result.get('confidence_score', 0.7)
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # æœ€çµ‚çš„ãªå›ç­”ã‚’æ§‹é€ åŒ–
        if not final_answer_content.startswith('#'):
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ãªã„å ´åˆã¯ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
            formatted_response = f"""# ğŸ” æ·±åº¦èª¿æŸ»çµæœ

**è³ªå•**: {question}  
**èª¿æŸ»å®Œäº†**: {current_time}  
**ä¿¡é ¼åº¦**: {confidence_score:.0%}

---

{final_answer_content}

---

*ã“ã®å›ç­”ã¯è¤‡æ•°ã®æœ€æ–°æƒ…å ±æºã‚’åŸºã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""
        else:
            # æ—¢ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            formatted_response = final_answer_content
        
        from langchain_core.messages import AIMessage
        return {"messages": [AIMessage(content=formatted_response)]}
        
    except Exception as e:
        print(f"Deep research error: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        from langchain_core.messages import AIMessage
        error_message = AIMessage(content=f"""# âŒ èª¿æŸ»ã‚¨ãƒ©ãƒ¼

ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€èª¿æŸ»ä¸­ã«æŠ€è¡“çš„ãªå•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„
- ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã«ã—ã¦ã„ãŸã ãã¨ã€ã‚ˆã‚Šè‰¯ã„çµæœãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™

## ğŸ“ åŸºæœ¬çš„ãªæƒ…å ±
2025å¹´ã®AIã«é–¢ã™ã‚‹ä¸€èˆ¬çš„ãªæƒ…å ±ã‚’ãŠæ¢ã—ã®å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼š
- ç”ŸæˆAIæŠ€è¡“ã®é€²æ­©
- AIã‚¬ãƒãƒŠãƒ³ã‚¹ã¨è¦åˆ¶
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AI
- AIå€«ç†ã¨å®‰å…¨æ€§

**ã‚¨ãƒ©ãƒ¼è©³ç´°**: `{str(e)}`

ä½•ã‹ã”ä¸æ˜ãªç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠå£°ãŒã‘ãã ã•ã„ã€‚""")
        return {"messages": [error_message]}

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("Deep Research ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚")
print("ä»¥ä¸‹ã®é«˜åº¦ãªæ©Ÿèƒ½ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼š")
print("- åŒ…æ‹¬çš„ãªç ”ç©¶è¨ˆç”»ç«‹æ¡ˆ")
print("- å¤šè§’çš„æƒ…å ±åé›†ï¼ˆæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹å«ã‚€ï¼‰")
print("- å°‚é–€å®¶è¦–ç‚¹ã®åˆ†æ")
print("- ç ”ç©¶ã‚®ãƒ£ãƒƒãƒ—ã®ç‰¹å®š")
print("- è©³ç´°ãªäº‹å®Ÿæ¤œè¨¼")
print("- åŒ…æ‹¬çš„ãªæƒ…å ±çµ±åˆ")
print("- æœ€çµ‚å›ç­”ç”Ÿæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
print("- åå¾©çš„æ·±åŒ–ãƒ—ãƒ­ã‚»ã‚¹")
print("- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°")
print("- Tavilyã‚’ä½¿ã£ãŸæœ€æ–°æƒ…å ±å–å¾—")
print(f"- æœ€å¤§{3}å›ã®ç ”ç©¶ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")


if __name__ == "__main__":
    question = "AIã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
    print(deep_research_agent(question))